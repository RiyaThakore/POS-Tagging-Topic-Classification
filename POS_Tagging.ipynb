{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS Tagging.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNELbGlzAxdIBskDqYWbE8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiyaThakore/POS-Tagging-Topic-Classification/blob/master/POS_Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHKOkUMHDxMO",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ysbpzl6tPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1266218d-ded3-4961-c2bb-88aa198a3a4a"
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> all\n",
            "Command 'all' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Download\n",
            "Command 'Download' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> x\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y13M4IMa7wOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d7e1693e-ccd1-4da1-83cb-26516b9f431c"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgkFoxuiD2Li",
        "colab_type": "text"
      },
      "source": [
        "Load Data and Create POS Taggers List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SojISDTf5nLt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "outputId": "4e0b8c54-a0a0-4706-e295-0d6bbcb8b4f1"
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "stop_words = set(stopwords.words('english')) \n",
        " \n",
        "txt = open('/content/sample_data/NewsDataset.txt').read()\n",
        "tokenized = sent_tokenize(txt) \n",
        "for i in tokenized: \n",
        "    wordsList = nltk.word_tokenize(i) \n",
        "    wordsList = [w for w in wordsList if not w in stop_words] \n",
        "    tagged = nltk.pos_tag(wordsList) \n",
        "    print(tagged) \n",
        "pos=[]\n",
        "for i in range(0,9):\n",
        "  pos.append(tagged[i][1])\n",
        "\n",
        "print(list(set(pos)))\n",
        "#print(pos)\n",
        "'''\n",
        "RB -> Adverb\n",
        "CD -> Digit\n",
        "VBG,VBD, VB, MD -> Verb\n",
        "NN -> Noun\n",
        "JJ -> Adjective\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Story', 'NNP'), ('highlights', 'NNS'), ('Ronna', 'NNP'), ('McDaniel', 'NNP'), (':', ':'), ('Tuesday', 'NNP'), ('night', 'NN'), (',', ','), ('President', 'NNP'), ('Trump', 'NNP'), ('spoke', 'VBD'), ('directly', 'RB'), ('American', 'JJ'), ('people', 'NNS'), ('heart', 'NN'), ('Despite', 'IN'), ('party', 'NN'), ('differences', 'NNS'), (',', ','), ('Democrats', 'NNPS'), (',', ','), (',', ','), ('duty', 'NN'), ('get', 'VB'), ('board', 'NN'), ('repair', 'NN'), ('wreckage', 'NN'), ('failed', 'VBD'), ('leadership', 'NN'), (',', ','), ('writes', 'VBZ'), ('Ronna', 'NNP'), ('McDaniel', 'NNP'), ('second', 'JJ'), ('woman', 'NN'), ('ever', 'RB'), ('elected', 'VBN'), ('chairwoman', 'JJ'), ('Republican', 'NNP'), ('National', 'NNP'), ('Committee', 'NNP'), ('.', '.')]\n",
            "[('She', 'PRP'), ('elected', 'VBD'), ('state', 'NN'), ('chairman', 'NN'), ('Michigan', 'NNP'), ('February', 'NNP'), ('2015', 'CD'), ('.', '.')]\n",
            "[('In', 'IN'), ('community', 'NN'), ('Northville', 'NNP'), (',', ','), ('McDaniel', 'NNP'), ('served', 'VBD'), ('land', 'NN'), ('planning', 'VBG'), ('public', 'JJ'), ('safety', 'NN'), ('committees', 'NNS'), ('actively', 'RB'), ('involved', 'VBD'), ('local', 'JJ'), ('PTA', 'NNP'), ('.', '.')]\n",
            "[('The', 'DT'), ('views', 'NNS'), ('expressed', 'VBD'), ('commentary', 'JJ'), ('solely', 'RB'), ('author', 'NN'), ('.', '.')]\n",
            "[('(', '('), ('CNN', 'NNP'), (')', ')'), ('This', 'DT'), ('week', 'NN'), (',', ','), ('President', 'NNP'), ('Donald', 'NNP'), ('Trump', 'NNP'), ('made', 'VBD'), ('history', 'NN'), ('joint', 'NN'), ('address', 'NN'), ('Congress', 'NNP'), ('.', '.')]\n",
            "[('He', 'PRP'), ('painted', 'VBD'), ('picture', 'NN'), ('united', 'JJ'), ('America', 'NNP'), (',', ','), ('one', 'CD'), ('renewing', 'VBG'), ('hope', 'NN'), ('country', 'NN'), ('prosper', 'NN'), ('America', 'NNP'), ('first', 'JJ'), ('policies', 'NNS'), ('.', '.')]\n",
            "[('Even', 'RB'), ('stood', 'VBD'), ('House', 'NNP'), ('chamber', 'NN'), (',', ','), ('President', 'NNP'), ('Trump', 'NNP'), ('spoke', 'VBD'), ('directly', 'RB'), ('American', 'JJ'), ('people', 'NNS'), ('heart', 'NN'), (',', ','), ('addressed', 'VBD'), ('many', 'JJ'), ('issues', 'NNS'), ('important', 'JJ'), ('.', '.')]\n",
            "[('He', 'PRP'), ('redoubled', 'VBD'), ('commitment', 'JJ'), ('protecting', 'VBG'), ('country', 'NN'), ('fighting', 'VBG'), ('international', 'JJ'), ('homegrown', 'JJ'), ('terrorism', 'NN'), (',', ','), ('securing', 'VBG'), ('borders', 'NNS'), (',', ','), ('rebuilding', 'VBG'), ('relationships', 'NNS'), ('allies', 'NNS'), ('around', 'IN'), ('world', 'NN'), ('.', '.')]\n",
            "[('And', 'CC'), ('memorable', 'JJ'), ('gesture', 'NN'), ('reverence', 'NN'), (',', ','), ('led', 'VBD'), ('room', 'NN'), ('nation', 'NN'), (\"'s\", 'POS'), ('leaders', 'NNS'), ('honoring', 'VBG'), ('legacy', 'NN'), ('Ryan', 'NNP'), ('Owens', 'NNP'), (',', ','), ('paid', 'VBD'), ('ultimate', 'JJ'), ('sacrifice', 'JJ'), ('national', 'JJ'), ('security', 'NN'), ('.', '.')]\n",
            "[('Ronna', 'NNP'), ('McDaniel', 'NNP'), ('He', 'PRP'), ('extended', 'VBD'), ('hand', 'NN'), ('across', 'IN'), ('aisle', 'NN'), ('issues', 'NNS'), ('rebuilding', 'VBG'), ('American', 'JJ'), ('infrastructure', 'NN'), ('emphasizing', 'VBG'), ('importance', 'NN'), ('paid', 'VBN'), ('family', 'NN'), ('leave', 'NN'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('President', 'NNP'), ('understands', 'VBZ'), ('duty', 'JJ'), ('American', 'JJ'), ('people', 'NNS'), ('purpose', 'VBP'), ('help', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('purpose', 'NN'), ('always', 'RB'), ('greater', 'JJR'), ('partisanship', 'NN'), ('.', '.')]\n",
            "[('Unfortunately', 'RB'), (',', ','), ('Democrats', 'NNPS'), ('far', 'RB'), ('remain', 'VBP'), ('unmoved', 'JJ'), ('cries', 'NNS'), ('across', 'IN'), ('country', 'NN'), ('find', 'VBP'), ('common', 'JJ'), ('ground', 'NN'), ('President', 'NNP'), ('.', '.')]\n",
            "[('Despite', 'IN'), ('party', 'NN'), ('differences', 'NNS'), (',', ','), (',', ','), (',', ','), ('duty', 'NN'), ('get', 'VB'), ('board', 'NN'), ('help', 'NN'), ('repair', 'NN'), ('wreckage', 'NN'), ('left', 'VBD'), ('wake', 'VBP'), ('nearly', 'RB'), ('decade', 'NN'), ('failed', 'VBD'), ('leadership', 'NN'), ('.', '.')]\n",
            "[('Farmers', 'NNS'), ('eastern', 'JJ'), ('Kentucky', 'NNP'), (',', ','), ('coal', 'NN'), ('miners', 'NNS'), ('West', 'NNP'), ('Virginia', 'NNP'), ('factory', 'NN'), ('workers', 'NNS'), ('Pennsylvania', 'NNP'), ('play', 'VBP'), ('political', 'JJ'), ('games', 'NNS'), ('Washington', 'NNP'), ('elites', 'VBZ'), ('.', '.')]\n",
            "[('They', 'PRP'), ('see', 'VBP'), ('jobs', 'NNS'), ('moving', 'VBG'), ('overseas', 'RB'), ('loved', 'VBN'), ('ones', 'NNS'), ('crushed', 'VBD'), ('burdensome', 'JJ'), ('costs', 'NNS'), ('Obamacare', 'NNP'), ('.', '.')]\n",
            "[('These', 'DT'), ('friends', 'NNS'), ('neighbors', 'NNS'), (',', ','), ('need', 'VBP'), ('results', 'NNS'), ('--', ':'), ('rhetoric', 'NN'), ('.', '.')]\n",
            "[('Read', 'VB'), ('More', 'JJR'), ('How', 'NNP'), ('Trump', 'NNP'), ('pulled', 'VBD'), ('together', 'RB'), ('presidential', 'JJ'), ('moment', 'NN'), ('Tuesday', 'NNP'), (\"'s\", 'POS'), ('speech', 'NN'), ('one', 'CD'), ('illustration', 'NN'), ('President', 'NNP'), (\"'s\", 'POS'), ('broader', 'JJR'), ('commitment', 'NN'), ('helping', 'VBG'), ('workers', 'NNS'), ('.', '.')]\n",
            "[('Before', 'IN'), ('even', 'RB'), ('took', 'VBD'), ('office', 'NN'), (',', ','), ('President', 'NNP'), ('Trump', 'NNP'), ('began', 'VBD'), ('delivering', 'VBG'), ('results', 'NNS'), ('country', 'NN'), ('.', '.')]\n",
            "[('He', 'PRP'), ('went', 'VBD'), ('Indiana', 'NNP'), ('personally', 'RB'), ('rescue', 'VBZ'), ('jobs', 'NNS'), ('hundreds', 'NNS'), ('Carrier', 'NNP'), ('employees', 'NNS'), ('danger', 'NN'), ('losing', 'VBG'), ('foreign', 'JJ'), ('workers', 'NNS'), ('.', '.')]\n",
            "[('Other', 'JJ'), ('companies', 'NNS'), ('like', 'IN'), ('Dow', 'NNP'), ('Chemical', 'NNP'), (',', ','), ('based', 'VBN'), ('home', 'NN'), ('state', 'NN'), ('Michigan', 'NNP'), (',', ','), ('pledged', 'VBD'), ('reinvest', 'JJS'), ('American', 'JJ'), ('jobs', 'NNS'), ('.', '.')]\n",
            "[('Donald', 'NNP'), ('Trump', 'NNP'), (\"'s\", 'POS'), ('election', 'NN'), ('promise', 'NN'), ('cultivate', 'VBP'), ('economic', 'JJ'), ('growth', 'NN'), ('levels', 'NNS'), (\"n't\", 'RB'), ('seen', 'VBN'), ('decades', 'NNS'), ('clearly', 'RB'), ('making', 'VBG'), ('difference', 'NN'), ('.', '.')]\n",
            "[('After', 'IN'), ('years', 'NNS'), ('stagnant', 'JJ'), ('wages', 'NNS'), (',', ','), ('hope', 'VBP'), ('children', 'NNS'), ('shot', 'JJ'), ('American', 'JJ'), ('dream', 'NN'), ('promised', 'VBD'), ('parents', 'NNS'), (\"'\", 'POS'), ('generation', 'NN'), ('.', '.')]\n",
            "[('But', 'CC'), ('many', 'JJ'), ('children', 'NNS'), ('shot', 'VBP'), ('brighter', 'NN'), ('future', 'NN'), ('seems', 'VBZ'), ('unattainable', 'JJ'), ('quality', 'NN'), ('education', 'NN'), ('reach', 'NN'), ('.', '.')]\n",
            "[('As', 'IN'), ('mother', 'NN'), (',', ','), ('I', 'PRP'), ('understand', 'VBP'), ('tool', 'JJ'), ('important', 'JJ'), ('child', 'NN'), ('realizing', 'VBG'), ('full', 'JJ'), ('potential', 'JJ'), ('safe', 'JJ'), ('inspirational', 'JJ'), ('classroom', 'NN'), ('environment', 'NN'), ('.', '.')]\n",
            "[('This', 'DT'), ('something', 'NN'), ('President', 'NNP'), ('Trump', 'NNP'), ('understands', 'VBZ'), ('well', 'RB'), (',', ','), ('called', 'VBD'), ('Congress', 'NNP'), ('pass', 'NN'), ('education', 'NN'), ('bill', 'NN'), ('promotes', 'VBZ'), ('school', 'NN'), ('choice', 'NN'), ('children', 'NNS'), (',', ','), ('particularly', 'RB'), ('disadvantaged', 'VBD'), ('under-served', 'JJ'), ('communities', 'NNS'), ('.', '.')]\n",
            "[('The', 'DT'), ('President', 'NNP'), ('celebrated', 'VBD'), ('incredible', 'JJ'), ('story', 'NN'), ('Denisha', 'NNP'), ('Merriweather', 'NNP'), (',', ','), ('able', 'JJ'), ('break', 'NN'), ('cycle', 'NN'), ('poverty', 'NN'), ('private', 'JJ'), ('education', 'NN'), ('received', 'VBD'), ('type', 'JJ'), ('programs', 'NNS'), ('proposing', 'VBG'), ('.', '.')]\n",
            "[('Follow', 'NNP'), ('CNN', 'NNP'), ('Opinion', 'NNP'), ('Join', 'NNP'), ('us', 'PRP'), ('Twitter', 'NNP'), ('Facebook', 'NNP'), ('Stories', 'NNP'), ('like', 'IN'), ('Denisha', 'NNP'), (\"'s\", 'POS'), ('many', 'JJ'), ('President', 'NNP'), ('Trump', 'NNP'), ('leader', 'NN'), ('American', 'JJ'), ('people', 'NNS'), ('chose', 'VBD'), ('fight', 'JJ'), ('Washington', 'NNP'), ('.', '.')]\n",
            "[('His', 'PRP$'), ('bold', 'JJ'), ('message', 'NN'), ('unrelenting', 'JJ'), ('work', 'NN'), ('ethic', 'JJ'), ('encapsulate', 'JJ'), ('American', 'JJ'), ('spirit', 'NN'), ('resilience', 'NN'), ('optimism', 'NN'), ('better', 'RBR'), ('tomorrow', 'NN'), ('empowered', 'JJ'), ('nation', 'NN'), ('centuries', 'NNS'), ('.', '.')]\n",
            "[('I', 'PRP'), ('proud', 'VBP'), ('stand', 'VBP'), ('behind', 'IN'), ('implements', 'NNS'), ('vision', 'NN'), ('shared', 'VBN'), ('country', 'NN'), ('Tuesday', 'NNP'), ('night', 'NN'), ('.', '.')]\n",
            "['NNS', 'NNP', 'NN', 'VBN', 'PRP', 'IN', 'VBP']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'\\nRB -> Adverb\\nCD -> Digit\\nVBG,VBD, VB, MD -> Verb\\nNN -> Noun\\nJJ -> Adjective\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hthhGh2ZEDXP",
        "colab_type": "text"
      },
      "source": [
        "Differentiation into various tags\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGPZbv2a8NhB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "f018ea0e-1e5a-4b81-b356-1eea9b74ea77"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "noun = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('NN')]\n",
        "adjective = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('JJ')]\n",
        "verb = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('VBG') or pos.startswith('VBD') or pos.startswith('VB') or pos.startswith('MD')]\n",
        "adverb = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('RB')]\n",
        "pronouns = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('PRP')]\n",
        "determiners = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('DT')]\n",
        "preposition = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('IN')]\n",
        "digit = [word for word, pos in pos_tag(word_tokenize(txt)) if pos.startswith('CD')]\n",
        "print(list(set(noun)))\n",
        "print(len(noun))\n",
        "print(adjective)\n",
        "print(len(adjective))\n",
        "print(verb)\n",
        "print(len(verb))\n",
        "print(adverb)\n",
        "print(len(adverb))\n",
        "print(digit)\n",
        "print(len(digit))\n",
        "print(determiners)\n",
        "print(len(determiners))\n",
        "print(pronouns)\n",
        "print(len(pronouns))\n",
        "print(preposition)\n",
        "print(len(preposition))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['room', 'relationships', 'vision', 'sacrifice', 'picture', 'House', 'spirit', 'chairwoman', 'commentary', 'ones', 'games', 'state', 'hand', 'How', 'illustration', 'country', 'type', 'Tuesday', 'leadership', 'employees', 'costs', 'something', 'Stories', 'infrastructure', 'Merriweather', 'leaders', 'security', 'National', 'generation', 'Trump', 'poverty', 'Virginia', 'February', 'choice', 'Committee', 'purpose', 'decades', 'danger', 'Republican', 'Follow', 'Story', 'week', 'hope', 'Democrats', 'wages', 'partisanship', 'moment', 'terrorism', 'difference', 'neighbors', 'results', 'borders', 'land', 'parents', 'policies', 'promise', 'Denisha', 'coal', 'PTA', 'home', 'committees', 'Twitter', 'America', 'Carrier', 'wake', 'reverence', 'Farmers', 'Ryan', 'history', 'quality', 'party', 'leave', 'story', 'decade', 'Congress', 'chairman', 'author', 'cycle', 'resilience', 'work', 'tomorrow', 'Kentucky', 'environment', 'classroom', 'address', 'centuries', 'levels', 'community', 'board', 'world', 'bill', 'Join', 'Facebook', 'night', 'issues', 'dream', 'office', 'Washington', 'workers', 'duty', 'gesture', 'children', 'aisle', 'McDaniel', 'Opinion', 'programs', 'woman', 'importance', 'planning', 'nation', 'allies', 'Owens', 'growth', 'leader', 'safety', 'wreckage', 'tool', 'Ronna', 'future', 'differences', 'companies', 'communities', 'CNN', 'speech', 'Chemical', 'Obamacare', 'Michigan', 'Northville', 'legacy', 'ground', 'years', 'mother', 'President', 'shot', 'West', 'hundreds', 'highlights', 'reach', 'message', 'election', 'family', 'jobs', 'potential', 'views', 'cries', 'school', 'child', 'chamber', 'friends', 'Pennsylvania', 'heart', 'education', 'brighter', 'Dow', 'burdensome', 'commitment', 'miners', 'Indiana', 'people', 'Donald', 'optimism']\n",
            "223\n",
            "['American', 'failed', 'second', 'public', 'local', 'joint', 'united', 'first', 'American', 'many', 'important', 'international', 'homegrown', 'memorable', 'ultimate', 'national', 'such', 'American', 'American', 'greater', 'unmoved', 'common', 'failed', 'eastern', 'factory', 'political', 'loved', 'rhetoric', 'More', 'most', 'presidential', 'broader', 'foreign', 'Other', 'American', 'economic', 'stagnant', 'American', 'many', 'unattainable', 'important', 'full', 'safe', 'inspirational', 'disadvantaged', 'under-served', 'incredible', 'able', 'private', 'many', 'more', 'American', 'bold', 'unrelenting', 'ethic', 'American', 'better', 'proud']\n",
            "58\n",
            "['spoke', 'have', 'get', 'repair', 'writes', 'is', 'elected', 'was', 'elected', 'has', 'served', 'is', 'involved', 'expressed', 'are', 'made', 'painted', 'is', 'renewing', 'will', 'prosper', 'stood', 'spoke', 'addressed', 'are', 'redoubled', 'protecting', 'fighting', 'securing', 'rebuilding', 'led', 'honoring', 'paid', 'extended', 'rebuilding', 'emphasizing', 'paid', 'understands', 'is', 'is', 'help', 'should', 'be', 'remain', 'find', 'have', 'get', 'help', 'repair', 'left', 'do', 'play', 'elites', 'see', 'moving', 'being', 'crushed', 'are', 'need', 'Read', 'pulled', 'is', 'helping', 'took', 'began', 'delivering', 'went', 'rescue', 'were', 'losing', 'based', 'have', 'pledged', 'reinvest', 'cultivate', 'have', 'seen', 'is', 'making', 'is', 'hope', 'will', 'have', 'was', 'promised', 'seems', 'is', 'understand', 'is', 'realizing', 'is', 'understands', 'is', 'called', 'pass', 'promotes', 'celebrated', 'was', 'break', 'received', 'is', 'proposing', 'are', 'is', 'chose', 'fight', 'encapsulate', 'has', 'empowered', 'am', 'stand', 'implements', 'shared']\n",
            "113\n",
            "['directly', 'too', 'ever', 'actively', 'solely', 'Even', 'directly', 'always', 'Unfortunately', 'so', 'far', 'too', 'nearly', 'not', 'overseas', 'not', 'more', 'together', 'just', 'even', 'personally', \"n't\", 'clearly', 'once', 'again', 'so', 'more', 'as', 'well', 'particularly', 'so', 'behind']\n",
            "32\n",
            "['2015', 'one', 'one']\n",
            "3\n",
            "['the', 'a', 'the', 'the', 'the', 'the', 'The', 'this', 'those', 'the', 'This', 'a', 'a', 'a', 'the', 'the', 'both', 'the', 'a', 'a', 'the', 'the', 'a', 'the', 'the', 'the', 'This', 'the', 'the', 'a', 'the', 'the', 'a', 'the', 'the', 'These', 'the', 'a', 'a', 'the', 'a', 'a', 'a', 'a', 'no', 'a', 'a', 'This', 'an', 'all', 'those', 'The', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'a', 'this', 'the']\n",
            "62\n",
            "['his', 'she', 'She', 'her', 'her', 'his', 'He', 'he', 'his', 'them', 'He', 'his', 'our', 'our', 'our', 'he', 'our', 'our', 'He', 'Our', 'our', 'our', 'them', 'them', 'our', 'they', 'They', 'their', 'their', 'our', 'they', 'his', 'our', 'he', 'our', 'He', 'them', 'my', 'his', 'we', 'our', 'our', 'I', 'their', 'he', 'she', 'he', 'us', 'them', 'His', 'I', 'him', 'he', 'he', 'our']\n",
            "55\n",
            "['from', 'Despite', 'on', 'of', 'of', 'as', 'from', 'in', 'In', 'of', 'on', 'in', 'in', 'of', 'in', 'of', 'of', 'under', 'as', 'in', 'from', 'through', 'with', 'around', 'in', 'of', 'of', 'in', 'of', 'for', 'across', 'on', 'as', 'of', 'that', 'than', 'by', 'across', 'for', 'with', 'Despite', 'on', 'in', 'of', 'of', 'in', 'in', 'in', 'of', 'by', 'of', 'of', 'Before', 'for', 'of', 'of', 'in', 'of', 'like', 'in', 'of', 'in', 'at', 'in', 'After', 'of', 'that', 'at', 'for', 'at', 'because', 'out', 'of', 'As', 'that', 'than', 'on', 'for', 'in', 'of', 'of', 'because', 'of', 'through', 'of', 'on', 'like', 'for', 'in', 'of', 'for', 'for', 'as', 'with', 'on']\n",
            "95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4EuPsBaERBP",
        "colab_type": "text"
      },
      "source": [
        "Output into tabular and excel format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7eiLSaE81et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "dda3fff8-9c08-455e-bbee-12e9d45d55dd"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "t = PrettyTable(['POS Tagger', 'Data', 'Count'])\n",
        "t.add_row(['Noun', nouns, len(nouns)])\n",
        "t.add_row(['Adjective', adjective, len(adjective)])\n",
        "t.add_row(['Verb', list(set(verb)), len(verb)])\n",
        "t.add_row(['Adverb', adverb, len(adverb)])\n",
        "t.add_row(['Digit', digit, len(digit)])\n",
        "t.add_row(['pronouns', list(set(pronouns)), len(pronouns)])\n",
        "t.add_row(['determiners', list(set(determiners)), len(determiners)])\n",
        "t.add_row(['Preposition', list(set(preposition)), len(preposition)])\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
            "|  POS Tagger |                                                                                                                                                                                                                                                                                                                                                                                                                         Data                                                                                                                                                                                                                                                                                                                                                                                                                         | Count |\n",
            "+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
            "|     Noun    |                                                                                                                                                                                                                                                                                                                                             ['Sukanya', 'Rajib', 'Naba', 'friends.Sukanya', 'year', 'Marriage', 'step', '', 'life', 'friendship', 'bond', 'people.It', 'kind', 'love', 'friend', 'one']                                                                                                                                                                                                                                                                                                                                             |   16  |\n",
            "|  Adjective  |                                                                                       ['American', 'failed', 'second', 'public', 'local', 'joint', 'united', 'first', 'American', 'many', 'important', 'international', 'homegrown', 'memorable', 'ultimate', 'national', 'such', 'American', 'American', 'greater', 'unmoved', 'common', 'failed', 'eastern', 'factory', 'political', 'loved', 'rhetoric', 'More', 'most', 'presidential', 'broader', 'foreign', 'Other', 'American', 'economic', 'stagnant', 'American', 'many', 'unattainable', 'important', 'full', 'safe', 'inspirational', 'disadvantaged', 'under-served', 'incredible', 'able', 'private', 'many', 'more', 'American', 'bold', 'unrelenting', 'ethic', 'American', 'better', 'proud']                                                                                        |   58  |\n",
            "|     Verb    | ['losing', 'pass', 'hope', 'empowered', 'began', 'called', 'understands', 'do', 'seen', 'stand', 'understand', 'celebrated', 'find', 'stood', 'based', 'promised', 'am', 'is', 'making', 'have', 'realizing', 'rebuilding', 'delivering', 'left', 'elected', 'Read', 'shared', 'painted', 'received', 'will', 'prosper', 'has', 'writes', 'redoubled', 'elites', 'proposing', 'implements', 'took', 'fight', 'extended', 'emphasizing', 'play', 'need', 'should', 'remain', 'helping', 'chose', 'get', 'repair', 'addressed', 'protecting', 'expressed', 'led', 'see', 'paid', 'rescue', 'pledged', 'promotes', 'be', 'help', 'reinvest', 'renewing', 'pulled', 'made', 'securing', 'encapsulate', 'being', 'break', 'were', 'was', 'spoke', 'crushed', 'are', 'honoring', 'cultivate', 'seems', 'went', 'fighting', 'moving', 'involved', 'served'] |  113  |\n",
            "|    Adverb   |                                                                                                                                                                                                                                                                      ['directly', 'too', 'ever', 'actively', 'solely', 'Even', 'directly', 'always', 'Unfortunately', 'so', 'far', 'too', 'nearly', 'not', 'overseas', 'not', 'more', 'together', 'just', 'even', 'personally', \"n't\", 'clearly', 'once', 'again', 'so', 'more', 'as', 'well', 'particularly', 'so', 'behind']                                                                                                                                                                                                                                                                       |   32  |\n",
            "|    Digit    |                                                                                                                                                                                                                                                                                                                                                                                                                ['2015', 'one', 'one']                                                                                                                                                                                                                                                                                                                                                                                                                |   3   |\n",
            "|   pronouns  |                                                                                                                                                                                                                                                                                                                                                             ['He', 'they', 'Our', 'them', 'They', 'him', 'us', 'he', 'she', 'my', 'we', 'his', 'her', 'I', 'His', 'their', 'She', 'our']                                                                                                                                                                                                                                                                                                                                                             |   55  |\n",
            "| determiners |                                                                                                                                                                                                                                                                                                                                                                                   ['the', 'this', 'The', 'This', 'those', 'all', 'no', 'These', 'a', 'an', 'both']                                                                                                                                                                                                                                                                                                                                                                                   |   62  |\n",
            "| Preposition |                                                                                                                                                                                                                                                                                                                               ['around', 'like', 'under', 'After', 'from', 'Despite', 'as', 'than', 'by', 'because', 'with', 'through', 'on', 'Before', 'at', 'of', 'As', 'out', 'for', 'that', 'across', 'In', 'in']                                                                                                                                                                                                                                                                                                                                |   95  |\n",
            "+-------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AtXu0Wk-Jf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "cars = {'POS Tagger': ['Noun','Adjective','Verb','Adverb', 'Digit'],\n",
        "        'Data': [nouns,adjective, verb, adverb, digit],\n",
        "        'Count' : [len(nouns), len(adjective), len(verb), len(adverb), len(digit)]\n",
        "        }\n",
        "\n",
        "df = pd.DataFrame(cars, columns = ['POS Tagger', 'Data', 'Count'])\n",
        "\n",
        "df.to_excel (r'C:\\Users\\riyan\\Desktop\\export_dataframe.xlsx', index = False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}